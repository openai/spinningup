

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Key Papers in Deep RL &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Exercises" href="exercises.html" />
    <link rel="prev" title="Spinning Up as a Deep RL Researcher" href="spinningup.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction to RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rl_intro.html">Part 1: Key Concepts in RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro2.html">Part 2: Kinds of RL Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro3.html">Part 3: Intro to Policy Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Key Papers in Deep RL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-free-rl">1. Model-Free RL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a-deep-q-learning">a. Deep Q-Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-policy-gradients">b. Policy Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="#c-deterministic-policy-gradients">c. Deterministic Policy Gradients</a></li>
<li class="toctree-l3"><a class="reference internal" href="#d-distributional-rl">d. Distributional RL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#e-policy-gradients-with-action-dependent-baselines">e. Policy Gradients with Action-Dependent Baselines</a></li>
<li class="toctree-l3"><a class="reference internal" href="#f-path-consistency-learning">f. Path-Consistency Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#g-other-directions-for-combining-policy-learning-and-q-learning">g. Other Directions for Combining Policy-Learning and Q-Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#h-evolutionary-algorithms">h. Evolutionary Algorithms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#exploration">2. Exploration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a-intrinsic-motivation">a. Intrinsic Motivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-unsupervised-rl">b. Unsupervised RL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#transfer-and-multitask-rl">3. Transfer and Multitask RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hierarchy">4. Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory">5. Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-based-rl">6. Model-Based RL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a-model-is-learned">a. Model is Learned</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-model-is-given">b. Model is Given</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#meta-rl">7. Meta-RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scaling-rl">8. Scaling RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rl-in-the-real-world">9. RL in the Real World</a></li>
<li class="toctree-l2"><a class="reference internal" href="#safety">10. Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="#imitation-learning-and-inverse-reinforcement-learning">11. Imitation Learning and Inverse Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reproducibility-analysis-and-critique">12. Reproducibility, Analysis, and Critique</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bonus-classic-papers-in-rl-theory-or-review">13. Bonus: Classic Papers in RL Theory or Review</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">Exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Key Papers in Deep RL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/spinningup/keypapers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="key-papers-in-deep-rl">
<h1><a class="toc-backref" href="#id105">Key Papers in Deep RL</a><a class="headerlink" href="#key-papers-in-deep-rl" title="Permalink to this headline">¶</a></h1>
<p>What follows is a list of papers in deep RL that are worth reading. This is <em>far</em> from comprehensive, but should provide a useful starting point for someone looking to do research in the field.</p>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#key-papers-in-deep-rl" id="id105">Key Papers in Deep RL</a><ul>
<li><a class="reference internal" href="#model-free-rl" id="id106">1. Model-Free RL</a></li>
<li><a class="reference internal" href="#exploration" id="id107">2. Exploration</a></li>
<li><a class="reference internal" href="#transfer-and-multitask-rl" id="id108">3. Transfer and Multitask RL</a></li>
<li><a class="reference internal" href="#hierarchy" id="id109">4. Hierarchy</a></li>
<li><a class="reference internal" href="#memory" id="id110">5. Memory</a></li>
<li><a class="reference internal" href="#model-based-rl" id="id111">6. Model-Based RL</a></li>
<li><a class="reference internal" href="#meta-rl" id="id112">7. Meta-RL</a></li>
<li><a class="reference internal" href="#scaling-rl" id="id113">8. Scaling RL</a></li>
<li><a class="reference internal" href="#rl-in-the-real-world" id="id114">9. RL in the Real World</a></li>
<li><a class="reference internal" href="#safety" id="id115">10. Safety</a></li>
<li><a class="reference internal" href="#imitation-learning-and-inverse-reinforcement-learning" id="id116">11. Imitation Learning and Inverse Reinforcement Learning</a></li>
<li><a class="reference internal" href="#reproducibility-analysis-and-critique" id="id117">12. Reproducibility, Analysis, and Critique</a></li>
<li><a class="reference internal" href="#bonus-classic-papers-in-rl-theory-or-review" id="id118">13. Bonus: Classic Papers in RL Theory or Review</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="model-free-rl">
<h2><a class="toc-backref" href="#id106">1. Model-Free RL</a><a class="headerlink" href="#model-free-rl" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-deep-q-learning">
<h3>a. Deep Q-Learning<a class="headerlink" href="#a-deep-q-learning" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, Mnih et al, 2013. <strong>Algorithm: DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://arxiv.org/abs/1507.06527">Deep Recurrent Q-Learning for Partially Observable MDPs</a>, Hausknecht and Stone, 2015. <strong>Algorithm: Deep Recurrent Q-Learning.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>, Wang et al, 2015. <strong>Algorithm: Dueling DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>, Hasselt et al 2015. <strong>Algorithm: Double DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a>, Schaul et al, 2015. <strong>Algorithm: Prioritized Experience Replay (PER).</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.02298">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>, Hessel et al, 2017. <strong>Algorithm: Rainbow DQN.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="b-policy-gradients">
<h3>b. Policy Gradients<a class="headerlink" href="#b-policy-gradients" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><a class="reference external" href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a>, Mnih et al, 2016. <strong>Algorithm: A3C.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><a class="reference external" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>, Schulman et al, 2015. <strong>Algorithm: TRPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><a class="reference external" href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al, 2015. <strong>Algorithm: GAE.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>, Schulman et al, 2017. <strong>Algorithm: PPO-Clip, PPO-Penalty.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[11]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.02286">Emergence of Locomotion Behaviours in Rich Environments</a>, Heess et al, 2017. <strong>Algorithm: PPO-Penalty.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[12]</td><td><a class="reference external" href="https://arxiv.org/abs/1708.05144">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</a>, Wu et al, 2017. <strong>Algorithm: ACKTR.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[13]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.01224">Sample Efficient Actor-Critic with Experience Replay</a>, Wang et al, 2016. <strong>Algorithm: ACER.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[14]</td><td><a class="reference external" href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, Haarnoja et al, 2018. <strong>Algorithm: SAC.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="c-deterministic-policy-gradients">
<h3>c. Deterministic Policy Gradients<a class="headerlink" href="#c-deterministic-policy-gradients" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[15]</td><td><a class="reference external" href="http://proceedings.mlr.press/v32/silver14.pdf">Deterministic Policy Gradient Algorithms</a>, Silver et al, 2014. <strong>Algorithm: DPG.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[16]</td><td><a class="reference external" href="https://arxiv.org/abs/1509.02971">Continuous Control With Deep Reinforcement Learning</a>, Lillicrap et al, 2015. <strong>Algorithm: DDPG.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[17]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.09477">Addressing Function Approximation Error in Actor-Critic Methods</a>, Fujimoto et al, 2018. <strong>Algorithm: TD3.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="d-distributional-rl">
<h3>d. Distributional RL<a class="headerlink" href="#d-distributional-rl" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[18]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06887">A Distributional Perspective on Reinforcement Learning</a>, Bellemare et al, 2017. <strong>Algorithm: C51.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[19]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.10044">Distributional Reinforcement Learning with Quantile Regression</a>, Dabney et al, 2017. <strong>Algorithm: QR-DQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[20]</td><td><a class="reference external" href="https://arxiv.org/abs/1806.06923">Implicit Quantile Networks for Distributional Reinforcement Learning</a>, Dabney et al, 2018. <strong>Algorithm: IQN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[21]</td><td><a class="reference external" href="https://openreview.net/forum?id=ByG_3s09KX">Dopamine: A Research Framework for Deep Reinforcement Learning</a>, Anonymous, 2018. <strong>Contribution:</strong> Introduces Dopamine, a code repository containing implementations of DQN, C51, IQN, and Rainbow. <a class="reference external" href="https://github.com/google/dopamine">Code link.</a></td></tr>
</tbody>
</table>
</div>
<div class="section" id="e-policy-gradients-with-action-dependent-baselines">
<h3>e. Policy Gradients with Action-Dependent Baselines<a class="headerlink" href="#e-policy-gradients-with-action-dependent-baselines" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[22]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.02247">Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic</a>, Gu et al, 2016. <strong>Algorithm: Q-Prop.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[23]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.11198">Action-depedent Control Variates for Policy Optimization via Stein&#8217;s Identity</a>, Liu et al, 2017. <strong>Algorithm: Stein Control Variates.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[24]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.10031">The Mirage of Action-Dependent Baselines in Reinforcement Learning</a>, Tucker et al, 2018. <strong>Contribution:</strong> interestingly, critiques and reevaluates claims from earlier papers (including Q-Prop and stein control variates) and finds important methodological errors in them.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="f-path-consistency-learning">
<h3>f. Path-Consistency Learning<a class="headerlink" href="#f-path-consistency-learning" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id25" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[25]</td><td><a class="reference external" href="https://arxiv.org/abs/1702.08892">Bridging the Gap Between Value and Policy Based Reinforcement Learning</a>, Nachum et al, 2017. <strong>Algorithm: PCL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[26]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.01891">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a>, Nachum et al, 2017. <strong>Algorithm: Trust-PCL.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="g-other-directions-for-combining-policy-learning-and-q-learning">
<h3>g. Other Directions for Combining Policy-Learning and Q-Learning<a class="headerlink" href="#g-other-directions-for-combining-policy-learning-and-q-learning" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[27]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.01626">Combining Policy Gradient and Q-learning</a>, O&#8217;Donoghue et al, 2016. <strong>Algorithm: PGQL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[28]</td><td><a class="reference external" href="https://arxiv.org/abs/1704.04651">The Reactor: A Fast and Sample-Efficient Actor-Critic Agent for Reinforcement Learning</a>, Gruslys et al, 2017. <strong>Algorithm: Reactor.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[29]</td><td><a class="reference external" href="http://papers.nips.cc/paper/6974-interpolated-policy-gradient-merging-on-policy-and-off-policy-gradient-estimation-for-deep-reinforcement-learning">Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning</a>, Gu et al, 2017. <strong>Algorithm: IPG.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id30" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[30]</td><td><a class="reference external" href="https://arxiv.org/abs/1704.06440">Equivalence Between Policy Gradients and Soft Q-Learning</a>, Schulman et al, 2017. <strong>Contribution:</strong> Reveals a theoretical link between these two families of RL algorithms.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="h-evolutionary-algorithms">
<h3>h. Evolutionary Algorithms<a class="headerlink" href="#h-evolutionary-algorithms" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id31" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[31]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.03864">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, Salimans et al, 2017. <strong>Algorithm: ES.</strong></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="exploration">
<h2><a class="toc-backref" href="#id107">2. Exploration</a><a class="headerlink" href="#exploration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-intrinsic-motivation">
<h3>a. Intrinsic Motivation<a class="headerlink" href="#a-intrinsic-motivation" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id32" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[32]</td><td><a class="reference external" href="https://arxiv.org/abs/1605.09674">VIME: Variational Information Maximizing Exploration</a>, Houthooft et al, 2016. <strong>Algorithm: VIME.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[33]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.01868">Unifying Count-Based Exploration and Intrinsic Motivation</a>, Bellemare et al, 2016. <strong>Algorithm: CTS-based Pseudocounts.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id34" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[34]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01310">Count-Based Exploration with Neural Density Models</a>, Ostrovski et al, 2017. <strong>Algorithm: PixelCNN-based Pseudocounts.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[35]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.04717">#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a>, Tang et al, 2016. <strong>Algorithm: Hash-based Counts.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id36" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[36]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01260">EX2: Exploration with Exemplar Models for Deep Reinforcement Learning</a>, Fu et al, 2017. <strong>Algorithm: EX2.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id37" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[37]</td><td><a class="reference external" href="https://arxiv.org/abs/1705.05363">Curiosity-driven Exploration by Self-supervised Prediction</a>, Pathak et al, 2017. <strong>Algorithm: Intrinsic Curiosity Module (ICM).</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id38" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[38]</td><td><a class="reference external" href="https://arxiv.org/abs/1808.04355">Large-Scale Study of Curiosity-Driven Learning</a>, Burda et al, 2018. <strong>Contribution:</strong> Systematic analysis of how surprisal-based intrinsic motivation performs in a wide variety of environments.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[39]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.12894">Exploration by Random Network Distillation</a>, Burda et al, 2018. <strong>Algorithm: RND.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="b-unsupervised-rl">
<h3>b. Unsupervised RL<a class="headerlink" href="#b-unsupervised-rl" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id40" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[40]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.07507">Variational Intrinsic Control</a>, Gregor et al, 2016. <strong>Algorithm: VIC.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id41" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[41]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.06070">Diversity is All You Need: Learning Skills without a Reward Function</a>, Eysenbach et al, 2018. <strong>Algorithm: DIAYN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id42" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[42]</td><td><a class="reference external" href="https://arxiv.org/abs/1807.10299">Variational Option Discovery Algorithms</a>, Achiam et al, 2018. <strong>Algorithm: VALOR.</strong></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="transfer-and-multitask-rl">
<h2><a class="toc-backref" href="#id108">3. Transfer and Multitask RL</a><a class="headerlink" href="#transfer-and-multitask-rl" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id43" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[43]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.04671">Progressive Neural Networks</a>, Rusu et al, 2016. <strong>Algorithm: Progressive Networks.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id44" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[44]</td><td><a class="reference external" href="http://proceedings.mlr.press/v37/schaul15.pdf">Universal Value Function Approximators</a>, Schaul et al, 2015. <strong>Algorithm: UVFA.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id45" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[45]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.05397">Reinforcement Learning with Unsupervised Auxiliary Tasks</a>, Jaderberg et al, 2016. <strong>Algorithm: UNREAL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id46" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[46]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.03300">The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously</a>, Cabi et al, 2017. <strong>Algorithm: IU Agent.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id47" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[47]</td><td><a class="reference external" href="https://arxiv.org/abs/1701.08734">PathNet: Evolution Channels Gradient Descent in Super Neural Networks</a>, Fernando et al, 2017. <strong>Algorithm: PathNet.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id48" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[48]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.07907">Mutual Alignment Transfer Learning</a>, Wulfmeier et al, 2017. <strong>Algorithm: MATL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id49" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[49]</td><td><a class="reference external" href="https://openreview.net/forum?id=rk07ZXZRb&amp;noteId=rk07ZXZRb">Learning an Embedding Space for Transferable Robot Skills</a>, Hausman et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id50" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[50]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.01495">Hindsight Experience Replay</a>, Andrychowicz et al, 2017. <strong>Algorithm: Hindsight Experience Replay (HER).</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="hierarchy">
<h2><a class="toc-backref" href="#id109">4. Hierarchy</a><a class="headerlink" href="#hierarchy" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id51" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[51]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.04695">Strategic Attentive Writer for Learning Macro-Actions</a>, Vezhnevets et al, 2016. <strong>Algorithm: STRAW.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id52" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[52]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01161">FeUdal Networks for Hierarchical Reinforcement Learning</a>, Vezhnevets et al, 2017. <strong>Algorithm: Feudal Networks</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id53" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[53]</td><td><a class="reference external" href="https://arxiv.org/abs/1805.08296">Data-Efficient Hierarchical Reinforcement Learning</a>, Nachum et al, 2018. <strong>Algorithm: HIRO.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="memory">
<h2><a class="toc-backref" href="#id110">5. Memory</a><a class="headerlink" href="#memory" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id54" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[54]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.04460">Model-Free Episodic Control</a>, Blundell et al, 2016. <strong>Algorithm: MFEC.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id55" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[55]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.01988">Neural Episodic Control</a>, Pritzel et al, 2017. <strong>Algorithm: NEC.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id56" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[56]</td><td><a class="reference external" href="https://arxiv.org/abs/1702.08360">Neural Map: Structured Memory for Deep Reinforcement Learning</a>, Parisotto and Salakhutdinov, 2017. <strong>Algorithm: Neural Map.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id57" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[57]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.10760">Unsupervised Predictive Memory in a Goal-Directed Agent</a>, Wayne et al, 2018. <strong>Algorithm: MERLIN.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id58" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[58]</td><td><a class="reference external" href="https://arxiv.org/abs/1806.01822">Relational Recurrent Neural Networks</a>, Santoro et al, 2018. <strong>Algorithm: RMC.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="model-based-rl">
<h2><a class="toc-backref" href="#id111">6. Model-Based RL</a><a class="headerlink" href="#model-based-rl" title="Permalink to this headline">¶</a></h2>
<div class="section" id="a-model-is-learned">
<h3>a. Model is Learned<a class="headerlink" href="#a-model-is-learned" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id59" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[59]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06203">Imagination-Augmented Agents for Deep Reinforcement Learning</a>, Weber et al, 2017. <strong>Algorithm: I2A.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id60" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[60]</td><td><a class="reference external" href="https://arxiv.org/abs/1708.02596">Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning</a>, Nagabandi et al, 2017. <strong>Algorithm: MBMF.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id61" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[61]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.00101">Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning</a>, Feinberg et al, 2018. <strong>Algorithm: MVE.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id62" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[62]</td><td><a class="reference external" href="https://arxiv.org/abs/1807.01675">Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion</a>, Buckman et al, 2018. <strong>Algorithm: STEVE.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id63" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[63]</td><td><a class="reference external" href="https://openreview.net/forum?id=SJJinbWRZ&amp;noteId=SJJinbWRZ">Model-Ensemble Trust-Region Policy Optimization</a>, Kurutach et al, 2018. <strong>Algorithm: ME-TRPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id64" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[64]</td><td><a class="reference external" href="https://arxiv.org/abs/1809.05214">Model-Based Reinforcement Learning via Meta-Policy Optimization</a>, Clavera et al, 2018. <strong>Algorithm: MB-MPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id65" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[65]</td><td><a class="reference external" href="https://arxiv.org/abs/1809.01999">Recurrent World Models Facilitate Policy Evolution</a>, Ha and Schmidhuber, 2018.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="b-model-is-given">
<h3>b. Model is Given<a class="headerlink" href="#b-model-is-given" title="Permalink to this headline">¶</a></h3>
<table class="docutils footnote" frame="void" id="id66" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[66]</td><td><a class="reference external" href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a>, Silver et al, 2017. <strong>Algorithm: AlphaZero.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id67" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[67]</td><td><a class="reference external" href="https://arxiv.org/abs/1705.08439">Thinking Fast and Slow with Deep Learning and Tree Search</a>, Anthony et al, 2017. <strong>Algorithm: ExIt.</strong></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="meta-rl">
<h2><a class="toc-backref" href="#id112">7. Meta-RL</a><a class="headerlink" href="#meta-rl" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id68" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[68]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.02779">RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning</a>, Duan et al, 2016. <strong>Algorithm: RL^2.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id69" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[69]</td><td><a class="reference external" href="https://arxiv.org/abs/1611.05763">Learning to Reinforcement Learn</a>, Wang et al, 2016.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id70" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[70]</td><td><a class="reference external" href="https://arxiv.org/abs/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a>, Finn et al, 2017. <strong>Algorithm: MAML.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id71" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[71]</td><td><a class="reference external" href="https://openreview.net/forum?id=B1DmUzWAW&amp;noteId=B1DmUzWAW">A Simple Neural Attentive Meta-Learner</a>, Mishra et al, 2018. <strong>Algorithm: SNAIL.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="scaling-rl">
<h2><a class="toc-backref" href="#id113">8. Scaling RL</a><a class="headerlink" href="#scaling-rl" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id72" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[72]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.02811">Accelerated Methods for Deep Reinforcement Learning</a>, Stooke and Abbeel, 2018. <strong>Contribution:</strong> Systematic analysis of parallelization in deep RL across algorithms.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id73" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[73]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.01561">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>, Espeholt et al, 2018. <strong>Algorithm: IMPALA.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id74" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[74]</td><td><a class="reference external" href="https://openreview.net/forum?id=H1Dy---0Z">Distributed Prioritized Experience Replay</a>, Horgan et al, 2018. <strong>Algorithm: Ape-X.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id75" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[75]</td><td><a class="reference external" href="https://openreview.net/forum?id=r1lyTjAqYX">Recurrent Experience Replay in Distributed Reinforcement Learning</a>, Anonymous, 2018. <strong>Algorithm: R2D2.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id76" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[76]</td><td><a class="reference external" href="https://arxiv.org/abs/1712.09381">RLlib: Abstractions for Distributed Reinforcement Learning</a>, Liang et al, 2017. <strong>Contribution:</strong> A scalable library of RL algorithm implementations. <a class="reference external" href="https://ray.readthedocs.io/en/latest/rllib.html">Documentation link.</a></td></tr>
</tbody>
</table>
</div>
<div class="section" id="rl-in-the-real-world">
<h2><a class="toc-backref" href="#id114">9. RL in the Real World</a><a class="headerlink" href="#rl-in-the-real-world" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id77" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[77]</td><td><a class="reference external" href="https://arxiv.org/abs/1809.07731">Benchmarking Reinforcement Learning Algorithms on Real-World Robots</a>, Mahmood et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id78" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[78]</td><td><a class="reference external" href="https://arxiv.org/abs/1808.00177">Learning Dexterous In-Hand Manipulation</a>, OpenAI, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id79" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[79]</td><td><a class="reference external" href="https://arxiv.org/abs/1806.10293">QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</a>, Kalashnikov et al, 2018. <strong>Algorithm: QT-Opt.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id80" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[80]</td><td><a class="reference external" href="https://arxiv.org/abs/1811.00260">Horizon: Facebook&#8217;s Open Source Applied Reinforcement Learning Platform</a>, Gauci et al, 2018.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="safety">
<h2><a class="toc-backref" href="#id115">10. Safety</a><a class="headerlink" href="#safety" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id81" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[81]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.06565">Concrete Problems in AI Safety</a>, Amodei et al, 2016. <strong>Contribution:</strong> establishes a taxonomy of safety problems, serving as an important jumping-off point for future research. We need to solve these!</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id82" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[82]</td><td><a class="reference external" href="https://arxiv.org/abs/1706.03741">Deep Reinforcement Learning From Human Preferences</a>, Christiano et al, 2017. <strong>Algorithm: LFP.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id83" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[83]</td><td><a class="reference external" href="https://arxiv.org/abs/1705.10528">Constrained Policy Optimization</a>, Achiam et al, 2017. <strong>Algorithm: CPO.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id84" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[84]</td><td><a class="reference external" href="https://arxiv.org/abs/1801.08757">Safe Exploration in Continuous Action Spaces</a>, Dalal et al, 2018. <strong>Algorithm: DDPG+Safety Layer.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id85" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[85]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.05173">Trial without Error: Towards Safe Reinforcement Learning via Human Intervention</a>, Saunders et al, 2017. <strong>Algorithm: HIRL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id86" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[86]</td><td><a class="reference external" href="https://arxiv.org/abs/1711.06782">Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>, Eysenbach et al, 2017. <strong>Algorithm: Leave No Trace.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="imitation-learning-and-inverse-reinforcement-learning">
<h2><a class="toc-backref" href="#id116">11. Imitation Learning and Inverse Reinforcement Learning</a><a class="headerlink" href="#imitation-learning-and-inverse-reinforcement-learning" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id87" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[87]</td><td><a class="reference external" href="http://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf">Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy</a>, Ziebart 2010. <strong>Contributions:</strong> Crisp formulation of maximum entropy IRL.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id88" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[88]</td><td><a class="reference external" href="https://arxiv.org/abs/1603.00448">Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization</a>, Finn et al, 2016. <strong>Algorithm: GCL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id89" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[89]</td><td><a class="reference external" href="https://arxiv.org/abs/1606.03476">Generative Adversarial Imitation Learning</a>, Ho and Ermon, 2016. <strong>Algorithm: GAIL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id90" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[90]</td><td><a class="reference external" href="https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills</a>, Peng et al, 2018. <strong>Algorithm: DeepMimic.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id91" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[91]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.00821">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</a>, Peng et al, 2018. <strong>Algorithm: VAIL.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id92" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[92]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.05017">One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL</a>, Le Paine et al, 2018. <strong>Algorithm: MetaMimic.</strong></td></tr>
</tbody>
</table>
</div>
<div class="section" id="reproducibility-analysis-and-critique">
<h2><a class="toc-backref" href="#id117">12. Reproducibility, Analysis, and Critique</a><a class="headerlink" href="#reproducibility-analysis-and-critique" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id93" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[93]</td><td><a class="reference external" href="https://arxiv.org/abs/1604.06778">Benchmarking Deep Reinforcement Learning for Continuous Control</a>, Duan et al, 2016. <strong>Contribution: rllab.</strong></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id94" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[94]</td><td><a class="reference external" href="https://arxiv.org/abs/1708.04133">Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control</a>, Islam et al, 2017.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id95" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[95]</td><td><a class="reference external" href="https://arxiv.org/abs/1709.06560">Deep Reinforcement Learning that Matters</a>, Henderson et al, 2017.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id96" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[96]</td><td><a class="reference external" href="https://arxiv.org/abs/1810.02525">Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods</a>, Henderson et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id97" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[97]</td><td><a class="reference external" href="https://arxiv.org/abs/1811.02553">Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?</a>, Ilyas et al, 2018.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id98" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[98]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.07055">Simple Random Search Provides a Competitive Approach to Reinforcement Learning</a>, Mania et al, 2018.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="bonus-classic-papers-in-rl-theory-or-review">
<h2><a class="toc-backref" href="#id118">13. Bonus: Classic Papers in RL Theory or Review</a><a class="headerlink" href="#bonus-classic-papers-in-rl-theory-or-review" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id99" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[99]</td><td><a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>, Sutton et al, 2000. <strong>Contributions:</strong> Established policy gradient theorem and showed convergence of policy gradient algorithm for arbitrary policy classes.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id100" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[100]</td><td><a class="reference external" href="http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf">An Analysis of Temporal-Difference Learning with Function Approximation</a>, Tsitsiklis and Van Roy, 1997. <strong>Contributions:</strong> Variety of convergence results and counter-examples for value-learning methods in RL.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id101" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[101]</td><td><a class="reference external" href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Neural-Netw-2008-21-682_4867%5b0%5d.pdf">Reinforcement Learning of Motor Skills with Policy Gradients</a>, Peters and Schaal, 2008. <strong>Contributions:</strong> Thorough review of policy gradient methods at the time, many of which are still serviceable descriptions of deep RL methods.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id102" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[102]</td><td><a class="reference external" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately Optimal Approximate Reinforcement Learning</a>, Kakade and Langford, 2002. <strong>Contributions:</strong> Early roots for monotonic improvement theory, later leading to theoretical justification for TRPO and other algorithms.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id103" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[103]</td><td><a class="reference external" href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">A Natural Policy Gradient</a>, Kakade, 2002. <strong>Contributions:</strong> Brought natural gradients into RL, later leading to TRPO, ACKTR, and several other methods in deep RL.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id104" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[104]</td><td><a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning</a>, Szepesvari, 2009. <strong>Contributions:</strong> Unbeatable reference on RL before deep RL, containing foundations and theoretical background.</td></tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="exercises.html" class="btn btn-neutral float-right" title="Exercises" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="spinningup.html" class="btn btn-neutral" title="Spinning Up as a Deep RL Researcher" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>