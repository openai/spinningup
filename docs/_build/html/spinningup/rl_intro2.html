

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Part 2: Kinds of RL Algorithms &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Part 3: Intro to Policy Optimization" href="rl_intro3.html" />
    <link rel="prev" title="Part 1: Key Concepts in RL" href="rl_intro.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction to RL</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="rl_intro.html">Part 1: Key Concepts in RL</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Part 2: Kinds of RL Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#a-taxonomy-of-rl-algorithms">A Taxonomy of RL Algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-free-vs-model-based-rl">Model-Free vs Model-Based RL</a></li>
<li class="toctree-l3"><a class="reference internal" href="#what-to-learn">What to Learn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-to-learn-in-model-free-rl">What to Learn in Model-Free RL</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#what-to-learn-in-model-based-rl">What to Learn in Model-Based RL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#links-to-algorithms-in-taxonomy">Links to Algorithms in Taxonomy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro3.html">Part 3: Intro to Policy Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="keypapers.html">Key Papers in Deep RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">Exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Part 2: Kinds of RL Algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/spinningup/rl_intro2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="part-2-kinds-of-rl-algorithms">
<h1><a class="toc-backref" href="#id19">Part 2: Kinds of RL Algorithms</a><a class="headerlink" href="#part-2-kinds-of-rl-algorithms" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#part-2-kinds-of-rl-algorithms" id="id19">Part 2: Kinds of RL Algorithms</a><ul>
<li><a class="reference internal" href="#a-taxonomy-of-rl-algorithms" id="id20">A Taxonomy of RL Algorithms</a></li>
<li><a class="reference internal" href="#links-to-algorithms-in-taxonomy" id="id21">Links to Algorithms in Taxonomy</a></li>
</ul>
</li>
</ul>
</div>
<p>Now that we&#8217;ve gone through the basics of RL terminology and notation, we can cover a little bit of the richer material: the landscape of algorithms in modern RL, and a description of the kinds of trade-offs that go into algorithm design.</p>
<div class="section" id="a-taxonomy-of-rl-algorithms">
<h2><a class="toc-backref" href="#id20">A Taxonomy of RL Algorithms</a><a class="headerlink" href="#a-taxonomy-of-rl-algorithms" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="id18">
<img alt="../_images/rl_algorithms_9_15.svg" src="../_images/rl_algorithms_9_15.svg" /><p class="caption"><span class="caption-text">A non-exhaustive, but useful taxonomy of algorithms in modern RL. <a class="reference internal" href="#citations-below">Citations below.</a></span></p>
</div>
<p>We&#8217;ll start this section with a disclaimer: it&#8217;s really quite hard to draw an accurate, all-encompassing taxonomy of algorithms in the modern RL space, because the modularity of algorithms is not well-represented by a tree structure. Also, to make something that fits on a page and is reasonably digestible in an introduction essay, we have to omit quite a bit of more advanced material (exploration, transfer learning, meta learning, etc). That said, our goals here are</p>
<ul class="simple">
<li>to highlight the most foundational design choices in deep RL algorithms about what to learn and how to learn it,</li>
<li>to expose the trade-offs in those choices,</li>
<li>and to place a few prominent modern algorithms into context with respect to those choices.</li>
</ul>
<div class="section" id="model-free-vs-model-based-rl">
<h3>Model-Free vs Model-Based RL<a class="headerlink" href="#model-free-vs-model-based-rl" title="Permalink to this headline">¶</a></h3>
<p>One of the most important branching points in an RL algorithm is the question of <strong>whether the agent has access to (or learns) a model of the environment</strong>. By a model of the environment, we mean a function which predicts state transitions and rewards.</p>
<p>The main upside to having a model is that <strong>it allows the agent to plan</strong> by thinking ahead, seeing what would happen for a range of possible choices, and explicitly deciding between its options. Agents can then distill the results from planning ahead into a learned policy. A particularly famous example of this approach is <a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a>. When this works, it can result in a substantial improvement in sample efficiency over methods that don&#8217;t have a model.</p>
<p>The main downside is that <strong>a ground-truth model of the environment is usually not available to the agent.</strong> If an agent wants to use a model in this case, it has to learn the model purely from experience, which creates several challenges. The biggest challenge is that bias in the model can be exploited by the agent, resulting in an agent which performs well with respect to the learned model, but behaves sub-optimally (or super terribly) in the real environment. Model-learning is fundamentally hard, so even intense effort&#8212;being willing to throw lots of time and compute at it&#8212;can fail to pay off.</p>
<p>Algorithms which use a model are called <strong>model-based</strong> methods, and those that don&#8217;t are called <strong>model-free</strong>. While model-free methods forego the potential gains in sample efficiency from using a model, they tend to be easier to implement and tune. As of the time of writing this introduction (September 2018), model-free methods are more popular and have been more extensively developed and tested than model-based methods.</p>
</div>
<div class="section" id="what-to-learn">
<h3>What to Learn<a class="headerlink" href="#what-to-learn" title="Permalink to this headline">¶</a></h3>
<p>Another critical branching point in an RL algorithm is the question of <strong>what to learn.</strong> The list of usual suspects includes</p>
<ul class="simple">
<li>policies, either stochastic or deterministic,</li>
<li>action-value functions (Q-functions),</li>
<li>value functions,</li>
<li>and/or environment models.</li>
</ul>
<div class="section" id="what-to-learn-in-model-free-rl">
<h4>What to Learn in Model-Free RL<a class="headerlink" href="#what-to-learn-in-model-free-rl" title="Permalink to this headline">¶</a></h4>
<p>There are two main approaches to representing and training agents with model-free RL:</p>
<p><strong>Policy Optimization.</strong> Methods in this family represent a policy explicitly as <img class="math" src="../_images/math/8bc2ffb416e6b729009dac19bf9efe3c144cc2ac.svg" alt="\pi_{\theta}(a|s)"/>. They optimize the parameters <img class="math" src="../_images/math/1a20bd03ccceae216a40cb69d3fb7a3970f6f275.svg" alt="\theta"/> either directly by gradient ascent on the performance objective <img class="math" src="../_images/math/bac2be784363161f5e76c206e3319aa035c066fc.svg" alt="J(\pi_{\theta})"/>,  or indirectly, by maximizing local approximations of <img class="math" src="../_images/math/bac2be784363161f5e76c206e3319aa035c066fc.svg" alt="J(\pi_{\theta})"/>. This optimization is almost always performed <strong>on-policy</strong>, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator <img class="math" src="../_images/math/9810155b30a8fc71eaed8c6f0f62a228c21f11ed.svg" alt="V_{\phi}(s)"/> for the on-policy value function <img class="math" src="../_images/math/3c343749cc2f7804a670b618d03d056d7e35b5eb.svg" alt="V^{\pi}(s)"/>, which gets used in figuring out how to update the policy.</p>
<p>A couple of examples of policy optimization methods are:</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1602.01783">A2C / A3C</a>, which performs gradient ascent to directly maximize performance,</li>
<li>and <a class="reference external" href="https://arxiv.org/abs/1707.06347">PPO</a>, whose updates indirectly maximize performance, by instead maximizing a <em>surrogate objective</em> function which gives a conservative estimate for how much <img class="math" src="../_images/math/bac2be784363161f5e76c206e3319aa035c066fc.svg" alt="J(\pi_{\theta})"/> will change as a result of the update.</li>
</ul>
<p><strong>Q-Learning.</strong> Methods in this family learn an approximator <img class="math" src="../_images/math/9f2416634a4ff217f555f9f37622c7d54cf9b9cb.svg" alt="Q_{\theta}(s,a)"/> for the optimal action-value function, <img class="math" src="../_images/math/3a641ac6d7de30796ff8c186c31692dd609123a0.svg" alt="Q^*(s,a)"/>. Typically they use an objective function based on the <a class="reference external" href="../spinningup/rl_intro.html#bellman-equations">Bellman equation</a>. This optimization is almost always performed <strong>off-policy</strong>, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained. The corresponding policy is obtained via the connection between <img class="math" src="../_images/math/27e0f032982b38571547cc19a81946ebdb69e1b9.svg" alt="Q^*"/> and <img class="math" src="../_images/math/3b90d2c0187600be023084b475e921927a91aa4f.svg" alt="\pi^*"/>: the actions taken by the Q-learning agent are given by</p>
<div class="math">
<p><img src="../_images/math/c6b731255918704ea4b9dd46d1ac8a098b476a92.svg" alt="a(s) = \arg \max_a Q_{\theta}(s,a)."/></p>
</div><p>Examples of Q-learning methods include</p>
<ul class="simple">
<li><a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a>, a classic which substantially launched the field of deep RL,</li>
<li>and <a class="reference external" href="https://arxiv.org/abs/1707.06887">C51</a>, a variant that learns a distribution over return whose expectation is <img class="math" src="../_images/math/27e0f032982b38571547cc19a81946ebdb69e1b9.svg" alt="Q^*"/>.</li>
</ul>
<p><strong>Trade-offs Between Policy Optimization and Q-Learning.</strong> The primary strength of policy optimization methods is that they are principled, in the sense that <em>you directly optimize for the thing you want.</em> This tends to make them stable and reliable. By contrast, Q-learning methods only <em>indirectly</em> optimize for agent performance, by training <img class="math" src="../_images/math/cd8d851b25e574efd2664f88ccddda3e165cb693.svg" alt="Q_{\theta}"/> to satisfy a self-consistency equation. There are many failure modes for this kind of learning, so it tends to be less stable. <a class="footnote-reference" href="#id2" id="id1">[1]</a> But, Q-learning methods gain the advantage of being substantially more sample efficient when they do work, because they can reuse data more effectively than policy optimization techniques.</p>
<p><strong>Interpolating Between Policy Optimization and Q-Learning.</strong> Serendipitously, policy optimization and Q-learning are not incompatible (and under some circumstances, it turns out, <a class="reference external" href="https://arxiv.org/abs/1704.06440">equivalent</a>), and there exist a range of algorithms that live in between the two extremes. Algorithms that live on this spectrum are able to carefully trade-off between the strengths and weaknesses of either side. Examples include</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1509.02971">DDPG</a>, an algorithm which concurrently learns a deterministic policy and a Q-function by using each to improve the other,</li>
<li>and <a class="reference external" href="https://arxiv.org/abs/1801.01290">SAC</a>, a variant which uses stochastic policies, entropy regularization, and a few other tricks to stabilize learning and score higher than DDPG on standard benchmarks.</li>
</ul>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>For more information about how and why Q-learning methods can fail, see 1) this classic paper by <a class="reference external" href="http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf">Tsitsiklis and van Roy</a>, 2) the (much more recent) <a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">review by Szepesvari</a> (in section 4.3.2), and 3) chapter 11 of <a class="reference external" href="https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view">Sutton and Barto</a>, especially section 11.3 (on &#8220;the deadly triad&#8221; of function approximation, bootstrapping, and off-policy data, together causing instability in value-learning algorithms).</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="what-to-learn-in-model-based-rl">
<h3>What to Learn in Model-Based RL<a class="headerlink" href="#what-to-learn-in-model-based-rl" title="Permalink to this headline">¶</a></h3>
<p>Unlike model-free RL, there aren&#8217;t a small number of easy-to-define clusters of methods for model-based RL: there are many orthogonal ways of using models. We&#8217;ll give a few examples, but the list is far from exhaustive. In each case, the model may either be given or learned.</p>
<p><strong>Background: Pure Planning.</strong> The most basic approach <em>never</em> explicitly represents the policy, and instead, uses pure planning techniques like <a class="reference external" href="https://en.wikipedia.org/wiki/Model_predictive_control">model-predictive control</a> (MPC) to select actions. In MPC, each time the agent observes the environment, it computes a plan which is optimal with respect to the model, where the plan describes all actions to take over some fixed window of time after the present. (Future rewards beyond the horizon may be considered by the planning algorithm through the use of a learned value function.) The agent then executes the first action of the plan, and immediately discards the rest of it. It computes a new plan each time it prepares to interact with the environment, to avoid using an action from a plan with a shorter-than-desired planning horizon.</p>
<ul class="simple">
<li>The <a class="reference external" href="https://sites.google.com/view/mbmf">MBMF</a> work explores MPC with learned environment models on some standard benchmark tasks for deep RL.</li>
</ul>
<p><strong>Expert Iteration.</strong> A straightforward follow-on to pure planning involves using and learning an explicit representation of the policy, <img class="math" src="../_images/math/8bc2ffb416e6b729009dac19bf9efe3c144cc2ac.svg" alt="\pi_{\theta}(a|s)"/>. The agent uses a planning algorithm (like Monte Carlo Tree Search) in the model, generating candidate actions for the plan by sampling from its current policy. The planning algorithm produces an action which is better than what the policy alone would have produced, hence it is an &#8220;expert&#8221; relative to the policy. The policy is afterwards updated to produce an action more like the planning algorithm&#8217;s output.</p>
<ul class="simple">
<li>The <a class="reference external" href="https://arxiv.org/abs/1705.08439">ExIt</a> algorithm uses this approach to train deep neural networks to play Hex.</li>
<li><a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a> is another example of this approach.</li>
</ul>
<p><strong>Data Augmentation for Model-Free Methods.</strong> Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use <em>only</em> fictitous experience for updating the agent.</p>
<ul class="simple">
<li>See <a class="reference external" href="https://arxiv.org/abs/1803.00101">MBVE</a> for an example of augmenting real experiences with fictitious ones.</li>
<li>See <a class="reference external" href="https://worldmodels.github.io/">World Models</a> for an example of using purely fictitious experience to train the agent, which they call &#8220;training in the dream.&#8221;</li>
</ul>
<p><strong>Embedding Planning Loops into Policies.</strong> Another approach embeds the planning procedure directly into a policy as a subroutine&#8212;so that complete plans become side information for the policy&#8212;while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.</p>
<ul class="simple">
<li>See <a class="reference external" href="https://arxiv.org/abs/1707.06203">I2A</a> for an example of agents being endowed with this style of imagination.</li>
</ul>
</div>
</div>
<div class="section" id="links-to-algorithms-in-taxonomy">
<h2><a class="toc-backref" href="#id21">Links to Algorithms in Taxonomy</a><a class="headerlink" href="#links-to-algorithms-in-taxonomy" title="Permalink to this headline">¶</a></h2>
<span class="target" id="citations-below"></span><table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td><a class="reference external" href="https://arxiv.org/abs/1602.01783">A2C / A3C</a> (Asynchronous Advantage Actor-Critic): Mnih et al, 2016</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06347">PPO</a> (Proximal Policy Optimization): Schulman et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td><a class="reference external" href="https://arxiv.org/abs/1502.05477">TRPO</a> (Trust Region Policy Optimization): Schulman et al, 2015</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td><a class="reference external" href="https://arxiv.org/abs/1509.02971">DDPG</a> (Deep Deterministic Policy Gradient): Lillicrap et al, 2015</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><a class="reference external" href="https://arxiv.org/abs/1802.09477">TD3</a> (Twin Delayed DDPG): Fujimoto et al, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td><a class="reference external" href="https://arxiv.org/abs/1801.01290">SAC</a> (Soft Actor-Critic): Haarnoja et al, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td><a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DQN</a> (Deep Q-Networks): Mnih et al, 2013</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[9]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06887">C51</a> (Categorical 51-Atom DQN): Bellemare et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[10]</td><td><a class="reference external" href="https://arxiv.org/abs/1710.10044">QR-DQN</a> (Quantile Regression DQN): Dabney et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[11]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.01495">HER</a> (Hindsight Experience Replay): Andrychowicz et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[12]</td><td><a class="reference external" href="https://worldmodels.github.io/">World Models</a>: Ha and Schmidhuber, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id14" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[13]</td><td><a class="reference external" href="https://arxiv.org/abs/1707.06203">I2A</a> (Imagination-Augmented Agents): Weber et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id15" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[14]</td><td><a class="reference external" href="https://sites.google.com/view/mbmf">MBMF</a> (Model-Based RL with Model-Free Fine-Tuning): Nagabandi et al, 2017</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[15]</td><td><a class="reference external" href="https://arxiv.org/abs/1803.00101">MBVE</a> (Model-Based Value Expansion): Feinberg et al, 2018</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[16]</td><td><a class="reference external" href="https://arxiv.org/abs/1712.01815">AlphaZero</a>: Silver et al, 2017</td></tr>
</tbody>
</table>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rl_intro3.html" class="btn btn-neutral float-right" title="Part 3: Intro to Policy Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rl_intro.html" class="btn btn-neutral" title="Part 1: Key Concepts in RL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>