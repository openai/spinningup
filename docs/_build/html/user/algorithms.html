

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Algorithms &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Running Experiments" href="running.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-s-included">What&#8217;s Included</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-these-algorithms">Why These Algorithms?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-on-policy-algorithms">The On-Policy Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-off-policy-algorithms">The Off-Policy Algorithms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#code-format">Code Format</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-algorithm-file">The Algorithm File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-core-file">The Core File</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction to RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">Part 1: Key Concepts in RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">Part 2: Kinds of RL Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">Part 3: Intro to Policy Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">Key Papers in Deep RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">Exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/user/algorithms.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="algorithms">
<h1><a class="toc-backref" href="#id1">Algorithms</a><a class="headerlink" href="#algorithms" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#algorithms" id="id1">Algorithms</a><ul>
<li><a class="reference internal" href="#what-s-included" id="id2">What&#8217;s Included</a></li>
<li><a class="reference internal" href="#why-these-algorithms" id="id3">Why These Algorithms?</a><ul>
<li><a class="reference internal" href="#the-on-policy-algorithms" id="id4">The On-Policy Algorithms</a></li>
<li><a class="reference internal" href="#the-off-policy-algorithms" id="id5">The Off-Policy Algorithms</a></li>
</ul>
</li>
<li><a class="reference internal" href="#code-format" id="id6">Code Format</a><ul>
<li><a class="reference internal" href="#the-algorithm-file" id="id7">The Algorithm File</a></li>
<li><a class="reference internal" href="#the-core-file" id="id8">The Core File</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="what-s-included">
<h2><a class="toc-backref" href="#id2">What&#8217;s Included</a><a class="headerlink" href="#what-s-included" title="Permalink to this headline">¶</a></h2>
<p>The following algorithms are implemented in the Spinning Up package:</p>
<ul class="simple">
<li><a class="reference external" href="../algorithms/vpg.html">Vanilla Policy Gradient</a> (VPG)</li>
<li><a class="reference external" href="../algorithms/trpo.html">Trust Region Policy Optimization</a> (TRPO)</li>
<li><a class="reference external" href="../algorithms/ppo.html">Proximal Policy Optimization</a> (PPO)</li>
<li><a class="reference external" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a> (DDPG)</li>
<li><a class="reference external" href="../algorithms/td3.html">Twin Delayed DDPG</a> (TD3)</li>
<li><a class="reference external" href="../algorithms/sac.html">Soft Actor-Critic</a> (SAC)</li>
</ul>
<p>They are all implemented with <a class="reference external" href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP</a> (non-recurrent) actor-critics, making them suitable for fully-observed, non-image-based RL environments, eg the <a class="reference external" href="https://gym.openai.com/envs/#mujoco">Gym Mujoco</a> environments.</p>
</div>
<div class="section" id="why-these-algorithms">
<h2><a class="toc-backref" href="#id3">Why These Algorithms?</a><a class="headerlink" href="#why-these-algorithms" title="Permalink to this headline">¶</a></h2>
<p>We chose the core deep RL algorithms in this package to reflect useful progressions of ideas from the recent history of the field, culminating in two algorithms in particular&#8212;PPO and SAC&#8212;which are close to SOTA on reliability and sample efficiency among policy-learning algorithms. They also expose some of the trade-offs that get made in designing and using algorithms in deep RL.</p>
<div class="section" id="the-on-policy-algorithms">
<h3><a class="toc-backref" href="#id4">The On-Policy Algorithms</a><a class="headerlink" href="#the-on-policy-algorithms" title="Permalink to this headline">¶</a></h3>
<p>Vanilla Policy Gradient is the most basic, entry-level algorithm in the deep RL space because it completely predates the advent of deep RL altogether. The core elements of VPG go all the way back to the late 80s / early 90s. It started a trail of research which ultimately led to stronger algorithms such as TRPO and then PPO soon after.</p>
<p>A key feature of this line of work is that all of these algorithms are <em>on-policy</em>: that is, they don&#8217;t use old data, which makes them weaker on sample efficiency. But this is for a good reason: these algorithms directly optimize the objective you care about&#8212;policy performance&#8212;and it works out mathematically that you need on-policy data to calculate the updates. So, this family of algorithms trades off sample efficiency in favor of stability&#8212;but you can see the progression of techniques (from VPG to TRPO to PPO) working to make up the deficit on sample efficiency.</p>
</div>
<div class="section" id="the-off-policy-algorithms">
<h3><a class="toc-backref" href="#id5">The Off-Policy Algorithms</a><a class="headerlink" href="#the-off-policy-algorithms" title="Permalink to this headline">¶</a></h3>
<p>DDPG is a similarly foundational algorithm to VPG, although much younger&#8212;the theory of deterministic policy gradients, which led to DDPG, wasn&#8217;t published until 2014. DDPG is closely connected to Q-learning algorithms, and it concurrently learns a Q-function and a policy which are updated to improve each other.</p>
<p>Algorithms like DDPG and Q-Learning are <em>off-policy</em>, so they are able to reuse old data very efficiently. They gain this benefit by exploiting Bellman&#8217;s equations for optimality, which a Q-function can be trained to satisfy using <em>any</em> environment interaction data (as long as there&#8217;s enough experience from the high-reward areas in the environment).</p>
<p>But problematically, there are no guarantees that doing a good job of satisfying Bellman&#8217;s equations leads to having great policy performance. <em>Empirically</em> one can get great performance&#8212;and when it happens, the sample efficiency is wonderful&#8212;but the absence of guarantees makes algorithms in this class potentially brittle and unstable. TD3 and SAC are descendants of DDPG which make use of a variety of insights to mitigate these issues.</p>
</div>
</div>
<div class="section" id="code-format">
<h2><a class="toc-backref" href="#id6">Code Format</a><a class="headerlink" href="#code-format" title="Permalink to this headline">¶</a></h2>
<p>All implementations in Spinning Up adhere to a standard template. They are split into two files: an algorithm file, which contains the core logic of the algorithm, and a core file, which contains various utilities needed to run the algorithm.</p>
<div class="section" id="the-algorithm-file">
<h3><a class="toc-backref" href="#id7">The Algorithm File</a><a class="headerlink" href="#the-algorithm-file" title="Permalink to this headline">¶</a></h3>
<p>The algorithm file always starts with a class definition for an experience buffer object, which is used to store information from agent-environment interactions.</p>
<p>Next, there is a single function which runs the algorithm, performing the following tasks (in this order):</p>
<blockquote>
<div><ol class="arabic simple">
<li>Logger setup</li>
<li>Random seed setting</li>
<li>Environment instantiation</li>
<li>Making placeholders for the computation graph</li>
<li>Building the actor-critic computation graph via the <code class="docutils literal"><span class="pre">actor_critic</span></code> function passed to the algorithm function as an argument</li>
<li>Instantiating the experience buffer</li>
<li>Building the computation graph for loss functions and diagnostics specific to the algorithm</li>
<li>Making training ops</li>
<li>Making the TF Session and initializing parameters</li>
<li>Setting up model saving through the logger</li>
<li>Defining functions needed for running the main loop of the algorithm (eg the core update function, get action function, and test agent function, depending on the algorithm)</li>
<li>Running the main loop of the algorithm:<ol class="loweralpha">
<li>Run the agent in the environment</li>
<li>Periodically update the parameters of the agent according to the main equations of the algorithm</li>
<li>Log key performance metrics and save agent</li>
</ol>
</li>
</ol>
</div></blockquote>
<p>Finally, there&#8217;s some support for directly running the algorithm in Gym environments from the command line.</p>
</div>
<div class="section" id="the-core-file">
<h3><a class="toc-backref" href="#id8">The Core File</a><a class="headerlink" href="#the-core-file" title="Permalink to this headline">¶</a></h3>
<p>The core files don&#8217;t adhere as closely as the algorithms files to a template, but do have some approximate structure:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Functions related to making and managing placeholders</li>
<li>Functions for building sections of computation graph relevant to the <code class="docutils literal"><span class="pre">actor_critic</span></code> method for a particular algorithm</li>
<li>Any other useful functions</li>
<li>Implementations for an MLP actor-critic compatible with the algorithm, where both the policy and the value function(s) are represented by simple MLPs</li>
</ol>
</div></blockquote>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="running.html" class="btn btn-neutral float-right" title="Running Experiments" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="installation.html" class="btn btn-neutral" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>